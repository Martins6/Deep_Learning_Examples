{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# pytorch framework\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, utils\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# pytorch-lightning framework\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# utils\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACT I\\nScene I. Athens. A room in the Palace of Theseus\\nScene II. The Same. A Room in a Cottage\\n\\nACT '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/shakespeare_midsummer_nights_dream.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "# filtering gutenberg's introduction\n",
    "text = text[945:]\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX 0 indices... [0 1 2 3 4 5 6 7 8 9]\n",
      "ENCODED ARR [72 14 47 40 86 83 62  0 15 16]\n",
      "...Our dataset\n",
      "x tensor([72, 14, 47, 40, 86, 83, 62,  0, 15, 16], dtype=torch.int32)\n",
      "y tensor([14, 47, 40, 86, 83, 62,  0, 15, 16, 15], dtype=torch.int32)\n",
      "\n",
      "\n",
      "INDEX 1 indices... [10 11 12 13 14 15 16 17 18 19]\n",
      "ENCODED ARR [15 40 86 77 40 72 46 59 15 16]\n",
      "...Our dataset\n",
      "x tensor([15, 40, 86, 77, 40, 72, 46, 59, 15, 16], dtype=torch.int32)\n",
      "y tensor([40, 86, 77, 40, 72, 46, 59, 15, 16, 70], dtype=torch.int32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        return torch.from_numpy(sample).int()\n",
    "\n",
    "class SequenceData(Dataset):\n",
    "    def __init__(self, encoded:np.array, seq_length_per_sample:int, transform=None):\n",
    "        self.encoded = encoded.copy()\n",
    "        self.transform = transform\n",
    "        self.seq_length_per_sample = seq_length_per_sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded)//self.seq_length_per_sample\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        K = self.seq_length_per_sample\n",
    "        sample = {}\n",
    "        sample['x'] = self.encoded[idx*K:((idx+1)*K)]\n",
    "        sample['y'] = self.encoded[idx*K:((idx+1)*K) + 1][1:]\n",
    "        if self.transform:\n",
    "            sample['x'] = self.transform(sample['x'])\n",
    "            sample['y'] = self.transform(sample['y'])\n",
    "        return sample\n",
    "\n",
    "K = 10\n",
    "seq_dataset = SequenceData(\n",
    "    encoded = encoded,\n",
    "    transform=ToTensor(),\n",
    "    seq_length_per_sample = K\n",
    ")\n",
    "\n",
    "for index, i in enumerate(seq_dataset):\n",
    "    print('INDEX', index, 'indices...', np.arange(index*K, ((index+1)*K)))\n",
    "    print('ENCODED ARR', encoded[index*K:((index+1)*K)])\n",
    "    y, x = i['y'], i['x']\n",
    "    print('...Our dataset')\n",
    "    print('x', x)\n",
    "    print('y', y)\n",
    "    print('\\n')\n",
    "    if index == 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length_per_sample = 100\n",
    "dataset_dict = {\n",
    "    'train': (\n",
    "        SequenceData(\n",
    "            encoded = encoded,\n",
    "            transform=ToTensor(),\n",
    "            seq_length_per_sample = seq_length_per_sample\n",
    "        )\n",
    "    ),\n",
    "    'val': (\n",
    "        SequenceData(\n",
    "            encoded = encoded[-1000:],\n",
    "            transform=ToTensor(),\n",
    "            seq_length_per_sample = seq_length_per_sample\n",
    "        )\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX 0 indices... [0 1 2 3 4 5 6 7 8 9]\n",
      "ENCODED ARR [72 14 47 40 86 83 62  0 15 16]\n",
      "...Our dataset\n",
      "x torch.Size([10, 100]) tensor([72, 14, 47, 40, 86, 83, 62,  0, 15, 16], dtype=torch.int32)\n",
      "y torch.Size([10, 100]) tensor([14, 47, 40, 86, 83, 62,  0, 15, 16, 15], dtype=torch.int32)\n",
      "\n",
      "\n",
      "INDEX 1 indices... [10 11 12 13 14 15 16 17 18 19]\n",
      "ENCODED ARR [15 40 86 77 40 72 46 59 15 16]\n",
      "...Our dataset\n",
      "x torch.Size([10, 100]) tensor([46, 15, 18, 70, 40,  4, 16, 40, 46, 59], dtype=torch.int32)\n",
      "y torch.Size([10, 100]) tensor([15, 18, 70, 40,  4, 16, 40, 46, 59, 15], dtype=torch.int32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pad_collate(batch):\n",
    "  (xx, yy) = [i['x'] for i in batch], [i['y'] for i in batch]\n",
    "  xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "  yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)\n",
    "\n",
    "  return {'x': xx_pad, 'y': yy_pad}\n",
    "\n",
    "seq_dataloader = DataLoader(\n",
    "    dataset_dict['train'],\n",
    "    batch_size=10,\n",
    "    num_workers=os.cpu_count(),\n",
    "    shuffle=False,\n",
    "    collate_fn=pad_collate\n",
    ")\n",
    "\n",
    "for index, sample in enumerate(seq_dataloader):\n",
    "    print('INDEX', index, 'indices...', np.arange(index*K, ((index+1)*K)))\n",
    "    print('ENCODED ARR', encoded[index*K:((index+1)*K)][0:10])\n",
    "    y, x = sample['y'], sample['x']\n",
    "    print('...Our dataset')\n",
    "    print('x', x.shape, x[0, :10])\n",
    "    print('y', y.shape, y[0, :10])\n",
    "    print('\\n')\n",
    "    if index == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX 0 indices... [0 1 2 3 4 5 6 7 8 9]\n",
      "ENCODED ARR [72 14 47 40 86 83 62  0 15 16]\n",
      "...Our dataset\n",
      "x torch.Size([10, 100]) tensor([72, 14, 47, 40, 86, 83, 62,  0, 15, 16], dtype=torch.int32)\n",
      "1 hot enc torch.Size([10, 100, 88]) torch.int64\n",
      "y torch.Size([10, 100]) tensor([14, 47, 40, 86, 83, 62,  0, 15, 16, 15], dtype=torch.int32)\n",
      "\n",
      "\n",
      "INDEX 1 indices... [10 11 12 13 14 15 16 17 18 19]\n",
      "ENCODED ARR [15 40 86 77 40 72 46 59 15 16]\n",
      "...Our dataset\n",
      "x torch.Size([10, 100]) tensor([46, 15, 18, 70, 40,  4, 16, 40, 46, 59], dtype=torch.int32)\n",
      "1 hot enc torch.Size([10, 100, 88]) torch.int64\n",
      "y torch.Size([10, 100]) tensor([15, 18, 70, 40,  4, 16, 40, 46, 59, 15], dtype=torch.int32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, sample in enumerate(seq_dataloader):\n",
    "    print('INDEX', index, 'indices...', np.arange(index*K, ((index+1)*K)))\n",
    "    print('ENCODED ARR', encoded[index*K:((index+1)*K)][0:10])\n",
    "    y, x = sample['y'], sample['x']\n",
    "    print('...Our dataset')\n",
    "    print('x', x.shape, x[0, :10])\n",
    "    print('1 hot enc', F.one_hot(x.long(), len(chars)).shape, F.one_hot(x.long(), len(chars)).dtype)\n",
    "    print('y', y.shape, y[0, :10])\n",
    "    print('\\n')\n",
    "    if index == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Missing logger folder: ../lightning_logs\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0     \n",
      "1 | lstm      | LSTM             | 3.3 M \n",
      "2 | dropout   | Dropout          | 0     \n",
      "3 | fc        | Linear           | 45.1 K\n",
      "-----------------------------------------------\n",
      "3.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.4 M     Total params\n",
      "13.517    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 24/24 [00:44<00:00,  1.87s/it, loss=2.59, v_num=0, val_loss=2.760]\n"
     ]
    }
   ],
   "source": [
    "class CharNN(pl.LightningModule):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, dataset_dict, dropout, char2int, int2char):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset_dict = dataset_dict\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.char2int = char2int\n",
    "        self.int2char = int2char\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hc = None\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_size, \n",
    "            hidden_size=self.hidden_dim, \n",
    "            num_layers=self.n_layers, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "    \n",
    "\n",
    "    def forward(self, x, hc=None):\n",
    "        if hc is None:\n",
    "            hc = self.hc\n",
    "\n",
    "        x = F.one_hot(x.long(), self.output_size).float()\n",
    "\n",
    "        # Because pytorch-lightning has a validation step where it train with a small dataset first...\n",
    "        if hc is not None and hc[0].shape[1] != x.shape[0]:\n",
    "            hc = None\n",
    "\n",
    "        outs, (h, c) = self.lstm(x, hc)\n",
    "\n",
    "        self.hc = (h.detach(),c.detach())\n",
    "        outs = outs.reshape(-1, self.hidden_dim)\n",
    "        x = self.fc(outs)\n",
    "        return x, (h,c)\n",
    "    \n",
    "    def predict(self, char, hc=None, top_k=None, temperature=1):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        x = torch.tensor([[self.char2int[char]]])\n",
    "        out, (h,c) = self.forward(x, hc)\n",
    "        out = out / temperature\n",
    "\n",
    "        p = F.softmax(out, dim=1).detach()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = torch.arange(self.output_size)\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.squeeze()\n",
    "            \n",
    "        p = p.squeeze().cpu().detach()\n",
    "        char = np.random.choice(top_ch.cpu().detach().numpy(), p=p.numpy())\n",
    "            \n",
    "        return self.int2char[char], (h,c)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['x'], batch['y']\n",
    "        y_hat, (h, c) = self(x)\n",
    "        loss = self.criterion(y_hat, y.flatten().long())\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch['x'], batch['y']\n",
    "        y_hat, (h, c) = self(x)\n",
    "        loss = self.criterion(y_hat, y.flatten().long())\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset_dict['train'],\n",
    "            batch_size=50,\n",
    "            num_workers=os.cpu_count(),\n",
    "            shuffle=False,\n",
    "            collate_fn=pad_collate\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset_dict['val'],\n",
    "            batch_size=50,\n",
    "            num_workers=os.cpu_count(),\n",
    "            shuffle=False,\n",
    "            collate_fn=pad_collate\n",
    "        )\n",
    "\n",
    "model = CharNN(\n",
    "    input_size=len(chars),\n",
    "    output_size=len(chars),\n",
    "    hidden_dim=512,\n",
    "    n_layers=2,\n",
    "    dataset_dict=dataset_dict,\n",
    "    dropout=0.3,\n",
    "    char2int=char2int,\n",
    "    int2char=int2char\n",
    "    \n",
    ")\n",
    "\n",
    "shutil.rmtree('../lightning_logs', ignore_errors=True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir='../',\n",
    "    log_every_n_steps=5,\n",
    "    max_epochs=10,\n",
    "    enable_progress_bar=True,\n",
    "    gradient_clip_val=5,\n",
    "    gradient_clip_algorithm=\"norm\"\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wers feeghe aktemu dlertotertand bumaclussinc\n",
      "pho:\n",
      "Pn ild me mounrt; lutoatirdgancmeteety; tfanmcoiggonmend anttu r se; onker, haliren ir kistoroanmorinlesI Dudpp umatt ot nutheit abguon is n! Dlipot s sothilt be tofet toise aelt eiSs.\n",
      "\n",
      "NDLRMTR, Hy Lx le:\n",
      "ou cewink le ikikn ler meseod’ sheu) I s aacTlidindiut tooveitheesmesgleiond sbead.\n",
      "DP[Oo ihe lim?\n",
      "\n",
      "YaOLSYPTEMASSTOEBETLEOIH.\n",
      "M. Dhine thad l’ fen lutitus, anhoce Rtoy I od monnslaatb cey are llanpaucn f allris; lruf and Etinlarh’t sorer cesey nnaa)um corheve tles feavd d batt sove tevt indeit  larurghed dhien pasnthaand,\n",
      "Te!\n",
      "H8nUEHELSEMMs d  met thuate Istasditin, fancethe pith  f ord piiret pemervorun™sage.B\n",
      ".IITO\n",
      "YTMEEtUfas ho tove huranhasthanl,,Were!:Yod carifoeknd imesu, ecsdin colru\n",
      "PDTOANLR\n",
      "OS.\n",
      "Tocat wusranthithead ugd ase nand fans.\n",
      "Weees. ™omcis ti the Qlrelepbe1taw ow wuinr, Tor sut aht on wouthitfeukf, pthed antefse lhyk tiqtoot thoiddemed w fholiro yhol I owlen ald ffof foDd kot. I  ligwcLiugb bil. Yhede Pn ondercocteifgaos\n"
     ]
    }
   ],
   "source": [
    "def generate_samples(net, n=100, hc=None, prime='We', top_k=None):\n",
    "    net.eval()\n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    for ch in prime:\n",
    "        char, hc = net.predict(ch, hc, top_k=top_k)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(n):\n",
    "        char, hc = net.predict(chars[-1], hc, top_k=top_k, temperature=1)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)\n",
    "\n",
    "print(generate_samples(model, n=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52acb0f760c37771d5ced520a9f86f781c7f33fc9dba82a14720cdcb640d5674"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('udacity-computer-vision-w1TboiOA-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
